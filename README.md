## Sparkify data warehouse project

## Project description:

Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
The analytics team is particularly interested in understanding what songs users are listening to.
This project consist in:

- creating a Postgres database with tables designed to optimize queries on song play analysis:
  creating a star schema database with fact table and dimension tables using sql_queries.py and create_tables.py 
  creating ETL pipeline to extract data from /data directory, transform data and load data into database tables using etl.ipynb and etl.py
   
- Testing the database and ETL pipeline by running queries given by the analytics team from Sparkify using test.ipynb

- compare my results with their expected results

### Database design:
### Schema for Song Play Analysis
Using the song and log datasets, we have created a star schema optimized for queries on song play analysis. This includes the following tables.

- Fact Table  
songplay: records in log data associated with song plays i.e. records with page NextSong
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
  
- Dimension Tables
users: users in the app
user_id, first_name, last_name, gender, level

song: songs in music database
song_id, title, artist_id, year, duration

artist: artists in music database
artist_id, name, location, latitude, longitude
  
time: timestamps of records in songplays broken down into specific units
start_time, hour, day, week, month, year, weekday


### ETL Process:

The etl.py file develops the ETL processes for each of the tables and load the whole datasets.

#### Process song_data:

In the first part of the etl.py file, we perform ETL on the first dataset, song_data, and load record into song and artist dimensional tables.
We proceed with  ETL on a single song file and load a single record into each table to start.

- We use the process_song_file() function to get a list of all song JSON files in data/song_data
- Select the first song in this list
- Read the song file and view the data 
- Insert artist record into artist table
- Insert song record into song table

#### Process log_data:

In this second part, we perform ETL on the second dataset, log_data, to load record into time and users dimensional tables, as well as the songplays fact table.

We perform ETL on a single log file and load a single record into each table.

- We use the process_log_file() function to get a list of all song JSON files in data/song_data
- Select the first log file in this list
- Read the log file and view the data
- Insert time record into time table
- Insert user record into users table
- Insert songplay record into songplay table


#### Process data:

In this third part, 
The process_data() function takes in argument the filepath and either the process_song_file or process_log_file function, 
and according to the arguments provided, ETL is performed on the whole dataset.

- Select all the files in the whole dataset provided
- lists all the files 
- iterate through the list
- Insert record into database tables according to the dataset provided.


The main() function runs the process_data() function for both datasets and close the connection.


### Project Repository files: This section describes what files are for which purpose in the project :

- Song dataset data/song_data/:
The first dataset  is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 
- Log dataset data/log_data/:
consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

In addition to the data files, the project workspace includes six files:

- test.ipynb displays the first few rows of each table to let us check our database.
- create_tables.py drops and creates your tables. You run this file to reset your tables before each time we run our ETL scripts.
- etl.ipynb reads and processes a single file from song_data and log_data and loads the data into our tables. This notebook contains detailed instructions on the ETL process for each of the tables.
- etl.py reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on our work in the ETL notebook.
- sql_queries.py contains all the sql queries, and is imported into the last three files above.
   Create Tables
   Write CREATE statements in sql_queries.py to create each table.
   Write DROP statements in sql_queries.py to drop each table if it exists.
- README.md provides discussion on the project.


### How To Run the Project?

- Run create_tables.py to create our database and tables.
- Develop ETL processes for each table. At the end of each table section, or at the end of the notebook, run test.ipynb to confirm that records were successfully inserted into each table.
- Remember to rerun create_tables.py to reset your tables before each time we run this notebook.
- Run etl.py 



